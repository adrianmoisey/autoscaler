{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is the documentation for the Vertical Pod Autoscaler.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See Installation for a quick tour that will get you started.</p>"},{"location":"faq/","title":"Vertical Pod Autoscaler FAQ","text":""},{"location":"faq/#contents","title":"Contents","text":"<ul> <li>VPA restarts my pods but does not modify CPU or memory settings. Why?</li> <li>How can I apply VPA to my Custom Resource?</li> <li>How can I use Prometheus as a history provider for the VPA recommender?</li> <li>I get recommendations for my single pod replicaSet, but they are not applied. Why?</li> <li>Can I run the VPA in an HA configuration?</li> <li>What are the parameters to VPA recommender?</li> <li>What are the parameters to VPA updater?</li> </ul>"},{"location":"faq/#vpa-restarts-my-pods-but-does-not-modify-cpu-or-memory-settings","title":"VPA restarts my pods but does not modify CPU or memory settings","text":"<p>First check that the VPA admission controller is running correctly:</p> <pre><code>$ kubectl get pod -n kube-system | grep vpa-admission-controller\nvpa-admission-controller-69645795dc-sm88s            1/1       Running   0          1m\n</code></pre> <p>Check the logs of the admission controller:</p> <p><code>$ kubectl logs -n kube-system vpa-admission-controller-69645795dc-sm88s</code></p> <p>If the admission controller is up and running, but there is no indication of it actually processing created pods or VPA objects in the logs, the webhook is not registered correctly.</p> <p>Check the output of:</p> <p><code>$ kubectl describe mutatingWebhookConfiguration vpa-webhook-config</code></p> <p>This should be correctly configured to point to the VPA admission webhook service. Example:</p> <pre><code>Name:         vpa-webhook-config\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  admissionregistration.k8s.io/v1beta1\nKind:         MutatingWebhookConfiguration\nMetadata:\n  Creation Timestamp:  2019-01-18T15:44:42Z\n  Generation:          1\n  Resource Version:    1250\n  Self Link:           /apis/admissionregistration.k8s.io/v1beta1/mutatingwebhookconfigurations/vpa-webhook-config\n  UID:                 f8ccd13d-1b37-11e9-8906-42010a84002f\nWebhooks:\n  Client Config:\n    Ca Bundle: &lt;redacted&gt;\n    Service:\n      Name:        vpa-webhook\n      Namespace:   kube-system\n  Failure Policy:  Ignore\n  Name:            vpa.k8s.io\n  Namespace Selector:\n  Rules:\n    API Groups:\n\n    API Versions:\n      v1\n    Operations:\n      CREATE\n    Resources:\n      pods\n    API Groups:\n      autoscaling.k8s.io\n    API Versions:\n      v1beta1\n    Operations:\n      CREATE\n      UPDATE\n    Resources:\n      verticalpodautoscalers\n</code></pre> <p>If the webhook config doesn't exist, something got wrong with webhook registration for admission controller. Check the logs for more info.</p> <p>From the above config following part defines the webhook service:</p> <pre><code>Service:\n  Name:        vpa-webhook\n  Namespace:   kube-system\n</code></pre> <p>Check that the service actually exists:</p> <p><code>$ kubectl describe -n kube-system service vpa-webhook</code></p> <pre><code>Name:              vpa-webhook\nNamespace:         kube-system\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=vpa-admission-controller\nType:              ClusterIP\nIP:                &lt;some_ip&gt;\nPort:              &lt;unset&gt;  443/TCP\nTargetPort:        8000/TCP\nEndpoints:         &lt;some_endpoint&gt;\nSession Affinity:  None\nEvents:            &lt;none&gt;\n</code></pre> <p>You can also curl the service's endpoint from within the cluster to make sure it is serving.</p> <p>Note: the commands will differ if you deploy VPA in a different namespace.</p>"},{"location":"faq/#how-can-i-apply-vpa-to-my-custom-resource","title":"How can I apply VPA to my Custom Resource?","text":"<p>The VPA can scale not only the built-in resources like Deployment or StatefulSet, but also Custom Resources which manage Pods. Just like the Horizontal Pod Autoscaler, the VPA requires that the Custom Resource implements the <code>/scale</code> subresource with the optional field <code>labelSelector</code>, which corresponds to <code>.scale.status.selector</code>. VPA doesn't use the <code>/scale</code> subresource for the actual scaling, but uses this label selector to identify the Pods managed by a Custom Resource. As VPA relies on Pod eviction to apply new resource recommendations, this ensures that all Pods with a matching VPA object are managed by a controller that will recreate them after eviction. Furthermore, it avoids misconfigurations that happened in the past when label selectors were specified manually.</p>"},{"location":"faq/#how-can-i-use-prometheus-as-a-history-provider-for-the-vpa-recommender","title":"How can I use Prometheus as a history provider for the VPA recommender","text":"<p>Configure your Prometheus to get metrics from cadvisor. Make sure that the metrics from the cadvisor have the label <code>job=kubernetes-cadvisor</code></p> <p>Set the flags <code>--storage=prometheus</code> and <code>--prometheus-address=&lt;your-prometheus-address&gt;</code> in the deployment for the <code>VPA recommender</code>. The <code>args</code> for the container should look something like this:</p> <pre><code>spec:\n  containers:\n  - args:\n    - --v=4\n    - --storage=prometheus\n    - --prometheus-address=http://prometheus.default.svc.cluster.local:9090\n</code></pre> <p>In this example, Prometheus is running in the default namespace.</p> <p>Now deploy the <code>VPA recommender</code> and check the logs.</p> <p><code>$ kubectl logs -n kube-system vpa-recommender-bb655b4b9-wk5x2</code></p> <p>Here you should see the flags that you set for the VPA recommender and you should see: <code>Initializing VPA from history provider</code></p> <p>This means that the VPA recommender is now using Prometheus as the history provider.</p>"},{"location":"faq/#i-get-recommendations-for-my-single-pod-replicaset-but-they-are-not-applied","title":"I get recommendations for my single pod replicaset but they are not applied","text":"<p>By default, the <code>--min-replicas</code> flag on the updater is set to 2. To change this, you can supply the arg in the deploys/updater-deployment.yaml file:</p> <pre><code>spec:\n  containers:\n  - name: updater\n    args:\n    - \"--min-replicas=1\"\n    - \"--v=4\"\n    - \"--stderrthreshold=info\"\n</code></pre> <p>and then deploy it manually if your vpa is already configured.</p>"},{"location":"faq/#can-i-run-the-vpa-in-an-ha-configuration","title":"Can I run the VPA in an HA configuration?","text":"<p>The VPA admission-controller can be run with multiple active Pods at any given time.</p> <p>Both the updater and recommender can only run a single active Pod at a time. Should you want to run a Deployment with more than one pod, it's recommended to enable a lease election with the <code>--leader-elect=true</code> parameter.</p> <p>NOTE: If using GKE, you must set <code>--leader-elect-resource-name</code> to something OTHER than \"vpa-recommender\", for example \"vpa-recommender-lease\".</p>"},{"location":"faq/#what-are-the-parameters-to-vpa-recommender","title":"What are the parameters to VPA recommender?","text":"<p>The following startup parameters are supported for VPA recommender:</p> <p>Name | Type | Description | Default |-|-|-|-| <code>recommendation-margin-fraction</code> | Float64 | Fraction of usage added as the safety margin to the recommended request | 0.15 <code>pod-recommendation-min-cpu-millicores</code> | Float64 | Minimum CPU recommendation for a pod | 25 <code>pod-recommendation-min-memory-mb</code> | Float64 | Minimum memory recommendation for a pod | 250 <code>target-cpu-percentile</code> | Float64 | CPU usage percentile that will be used as a base for CPU target recommendation | 0.9 <code>recommendation-lower-bound-cpu-percentile</code> | Float64 | CPU usage percentile that will be used for the lower bound on CPU recommendation | 0.5 <code>recommendation-upper-bound-cpu-percentile</code> | Float64 | CPU usage percentile that will be used for the upper bound on CPU recommendation | 0.95 <code>target-memory-percentile</code> | Float64 | Memory usage percentile that will be used as a base for memory target recommendation | 0.9 <code>recommendation-lower-bound-memory-percentile</code> | Float64 | Memory usage percentile that will be used for the lower bound on memory recommendation | 0.5 <code>recommendation-upper-bound-memory-percentile</code> | Float64 | Memory usage percentile that will be used for the upper bound on memory recommendation | 0.95 <code>checkpoints-timeout</code> | Duration | Timeout for writing checkpoints since the start of the recommender's main loop | time.Minute <code>min-checkpoints</code> | Int | Minimum number of checkpoints to write per recommender's main loop | 10 <code>memory-saver</code> | Bool | If true, only track pods which have an associated VPA | false <code>recommender-interval</code> | Duration | How often metrics should be fetched | 1*time.Minute <code>checkpoints-gc-interval</code> | Duration | How often orphaned checkpoints should be garbage collected | 10*time.Minute <code>prometheus-address</code> | String | Where to reach for Prometheus metrics | \"\" <code>prometheus-cadvisor-job-name</code> | String | Name of the prometheus job name which scrapes the cAdvisor metrics | \"kubernetes-cadvisor\" <code>address</code> | String | The address to expose Prometheus metrics. | \":8942\" <code>kubeconfig</code> | String | Path to a kubeconfig. Only required if out-of-cluster. | \"\" <code>kube-api-qps</code> | Float64 | QPS limit when making requests to Kubernetes apiserver | 5.0 <code>kube-api-burst</code> | Float64 | QPS burst limit when making requests to Kubernetes apiserver | 10.0 <code>storage</code> | String | Specifies storage mode. Supported values: prometheus, checkpoint (default) | \"\" <code>history-length</code> | String | How much time back prometheus have to be queried to get historical metrics | \"8d\" <code>history-resolution</code> | String | Resolution at which Prometheus is queried for historical metrics | \"1h\" <code>prometheus-query-timeout</code> | String | How long to wait before killing long queries | \"5m\" <code>pod-label-prefix</code> | String | Which prefix to look for pod labels in metrics | \"pod_label_\" <code>metric-for-pod-labels</code> | String | Which metric to look for pod labels in metrics | \"up{job=\\\"kubernetes-pods\\\"}\" <code>pod-namespace-label</code> | String | Label name to look for pod namespaces | \"kubernetes_namespace\" <code>pod-name-label</code> | String | Label name to look for pod names | \"kubernetes_pod_name\" <code>container-namespace-label</code> | String | Label name to look for container namespaces | \"namespace\" <code>container-pod-name-label</code> | String | Label name to look for container pod names | \"pod_name\" <code>container-name-label</code> | String | Label name to look for container names | \"name\" <code>vpa-object-namespace</code> | String | Namespace to search for VPA objects and pod stats. Empty means all namespaces will be used. | apiv1.NamespaceAll <code>memory-aggregation-interval</code> | Duration | The length of a single interval, for which the peak memory usage is computed. Memory usage peaks are aggregated in multiples of this interval. In other words there is one memory usage sample per interval (the maximum usage over that interval | model.DefaultMemoryAggregationInterval <code>memory-aggregation-interval-count</code> | Int64 | The number of consecutive memory-aggregation-intervals which make up the MemoryAggregationWindowLength which in turn is the period for memory usage aggregation by VPA. In other words, MemoryAggregationWindowLength = memory-aggregation-interval * memory-aggregation-interval-count. | model.DefaultMemoryAggregationIntervalCount <code>memory-histogram-decay-half-life</code> | Duration | The amount of time it takes a historical memory usage sample to lose half of its weight. In other words, a fresh usage sample is twice as 'important' as one with age equal to the half life period. | model.DefaultMemoryHistogramDecayHalfLife <code>cpu-histogram-decay-half-life</code> | Duration | The amount of time it takes a historical CPU usage sample to lose half of its weight. | model.DefaultCPUHistogramDecayHalfLife <code>cpu-integer-post-processor-enabled</code> | Bool | Enable the CPU integer recommendation post processor | false <code>leader-elect</code> | Bool | Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability. If enabling this in GKE, you MUST also manually set the <code>--leader-elect-resource-name</code> flag. | false <code>leader-elect-lease-duration</code> | Duration | The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled. | 15s <code>leader-elect-renew-deadline</code> | Duration | The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than the lease duration. This is only applicable if leader election is enabled. | 10s <code>leader-elect-resource-lock</code> | String | The type of resource object that is used for locking during leader election. Supported options are 'leases', 'endpointsleases' and 'configmapsleases'. | \"leases\" <code>leader-elect-resource-name</code> | String | The name of resource object that is used for locking during leader election. If using GKE, you must set this value to something OTHER than \"vpa-recommender\", for example \"vpa-recommender-lease\". | \"vpa-recommender\" <code>leader-elect-resource-namespace</code> | String | The namespace of resource object that is used for locking during leader election. | \"kube-system\" <code>leader-elect-retry-period</code> | Duration | The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled. | 2s</p>"},{"location":"faq/#what-are-the-parameters-to-vpa-updater","title":"What are the parameters to VPA updater?","text":"<p>The following startup parameters are supported for VPA updater:</p> <p>Name | Type | Description | Default |-|-|-|-| <code>pod-update-threshold</code> | Float64 | Ignore updates that have priority lower than the value of this flag | 0.1 <code>in-recommendation-bounds-eviction-lifetime-threshold</code> | Duration | Pods that live for at least that long can be evicted even if their request is within the [MinRecommended...MaxRecommended] range | time.Hour*12 <code>evict-after-oom-threshold</code> | Duration | Evict pod that has OOMed in less than evict-after-oom-threshold since start. | 10*time.Minute <code>updater-interval</code> | Duration | How often updater should run | 1*time.Minute <code>min-replicas</code> | Int | Minimum number of replicas to perform update | 2 <code>eviction-tolerance</code> | Float64 | Fraction of replica count that can be evicted for update, if more than one pod can be evicted. | 0.5 <code>eviction-rate-limit</code> | Float64 | Number of pods that can be evicted per seconds. A rate limit set to 0 or -1 will disable the rate limiter. | -1 <code>eviction-rate-burst</code> | Int | Burst of pods that can be evicted. | 1 <code>address</code> | String | The address to expose Prometheus metrics. | \":8943\" <code>kubeconfig</code> | String | Path to a kubeconfig. Only required if out-of-cluster. | \"\" <code>kube-api-qps</code> | Float64 | QPS limit when making requests to Kubernetes apiserver | 5.0 <code>kube-api-burst</code> | Float64 | QPS burst limit when making requests to Kubernetes apiserver | 10.0 <code>use-admission-controller-status</code> | Bool | If true, updater will only evict pods when admission controller status is valid. | true <code>vpa-object-namespace</code> | String | Namespace to search for VPA objects. Empty means all namespaces will be used. | apiv1.NamespaceAll <code>leader-elect</code> | Bool | Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability. | false <code>leader-elect-lease-duration</code> | Duration | The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled. | 15s <code>leader-elect-renew-deadline</code> | Duration | The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than the lease duration. This is only applicable if leader election is enabled. | 10s <code>leader-elect-resource-lock</code> | String | The type of resource object that is used for locking during leader election. Supported options are 'leases', 'endpointsleases' and 'configmapsleases'. | \"leases\" <code>leader-elect-resource-name</code> | String | The name of resource object that is used for locking during leader election. | \"vpa-updater\" <code>leader-elect-resource-namespace</code> | String | The namespace of resource object that is used for locking during leader election. | \"kube-system\" <code>leader-elect-retry-period</code> | Duration | The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled. | 2s</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#keeping-limit-proportional-to-request","title":"Keeping limit proportional to request","text":"<p>The container template specifies resource request for 500 milli CPU and 1 GB of RAM. The template also specifies resource limit of 2 GB RAM. VPA recommendation is 1000 milli CPU and 2 GB of RAM. When VPA applies the recommendation, it will also set the memory limit to 4 GB.</p>"},{"location":"examples/#capping-to-limit-range","title":"Capping to Limit Range","text":"<p>The container template specifies resource request for 500 milli CPU and 1 GB of RAM. The template also specifies resource limit of 2 GB RAM. A limit range sets a maximum limit to 3 GB RAM per container. VPA recommendation is 1000 milli CPU and 2 GB of RAM. When VPA applies the recommendation, it will set the memory limit to 3 GB (to keep it within the allowed limit range) and the memory request to 1.5 GB ( to maintain a 2:1 limit/request ratio from the template).</p>"},{"location":"examples/#resource-policy-overriding-limit-range","title":"Resource Policy Overriding Limit Range","text":"<p>The container template specifies resource request for 500 milli CPU and 1 GB of RAM. The template also specifies a resource limit of 2 GB RAM. A limit range sets a maximum limit to 3 GB RAM per container. VPAs Container Resource Policy requires VPA to set containers request to at least 750 milli CPU and 2 GB RAM. VPA recommendation is 1000 milli CPU and 2 GB of RAM. When applying the recommendation, VPA will set RAM request to 2 GB (following the resource policy) and RAM limit to 4 GB (to maintain the 2:1 limit/request ratio from the template).</p>"},{"location":"examples/#starting-multiple-recommenders","title":"Starting multiple recommenders","text":"<p>It is possible to start one or more extra recommenders in order to use different percentile on different workload profiles. For example you could have 3 profiles:  frugal, standard and performance which will use different TargetCPUPercentile (50, 90 and 95) to calculate their recommendations.</p> <p>Please note the usage of the following arguments to override default names and percentiles:</p> <ul> <li>--recommender-name=performance</li> <li>--target-cpu-percentile=0.95</li> </ul> <p>You can then choose which recommender to use by setting <code>recommenders</code> inside the <code>VerticalPodAutoscaler</code> spec.</p>"},{"location":"examples/#custom-memory-bump-up-after-oomkill","title":"Custom memory bump-up after OOMKill","text":"<p>After an OOMKill event was observed, VPA increases the memory recommendation based on the observed memory usage in the event according to this formula: <code>recommendation = memory-usage-in-oomkill-event + max(oom-min-bump-up-bytes, memory-usage-in-oomkill-event * oom-bump-up-ratio)</code>. You can configure the minimum bump-up as well as the multiplier by specifying startup arguments for the recommender: <code>oom-bump-up-ratio</code> specifies the memory bump up ratio when OOM occurred, default is <code>1.2</code>. This means, memory will be increased by 20% after an OOMKill event. <code>oom-min-bump-up-bytes</code> specifies minimal increase of memory after observing OOM. Defaults to <code>100 * 1024 * 1024</code> (=100MiB)</p> <p>Usage in recommender deployment</p> <pre><code>  containers:\n  - name: recommender\n    args:\n      - --oom-bump-up-ratio=2.0\n      - --oom-min-bump-up-bytes=524288000\n</code></pre>"},{"location":"examples/#using-cpu-management-with-static-policy","title":"Using CPU management with static policy","text":"<p>If you are using the CPU management with static policy for some containers, you probably want the CPU recommendation to be an integer. A dedicated recommendation pre-processor can perform a round up on the CPU recommendation. Recommendation capping still applies after the round up. To activate this feature, pass the flag <code>--cpu-integer-post-processor-enabled</code> when you start the recommender. The pre-processor only acts on containers having a specific configuration. This configuration consists in an annotation on your VPA object for each impacted container. The annotation format is the following:</p> <pre><code>vpa-post-processor.kubernetes.io/{containerName}_integerCPU=true\n</code></pre>"},{"location":"examples/#controlling-eviction-behavior-based-on-scaling-direction-and-resource","title":"Controlling eviction behavior based on scaling direction and resource","text":"<p>To limit disruptions caused by evictions, you can put additional constraints on the Updater's eviction behavior by specifying <code>.updatePolicy.EvictionRequirements</code> in the VPA spec. An <code>EvictionRequirement</code> contains a resource and a <code>ChangeRequirement</code>, which is evaluated by comparing a new recommendation against the currently set resources for a container</p> <p>Here is an example configuration which allows evictions only when CPU or memory get scaled up, but not when they both are scaled down</p> <pre><code> updatePolicy:\n   evictionRequirements:\n     - resources: [\"cpu\", \"memory\"]\n       changeRequirement: TargetHigherThanRequests\n</code></pre> <p>Note that this doesn't prevent scaling down entirely, as Pods may get recreated for different reasons, resulting in a new recommendation being applied. See the original AEP for more context and usage information.</p>"},{"location":"examples/#limiting-which-namespaces-are-used","title":"Limiting which namespaces are used","text":"<p>By default the VPA will run against all namespaces. You can limit that behaviour by setting the following options:</p> <ol> <li><code>ignored-vpa-object-namespaces</code> - A comma separated list of namespaces to ignore</li> <li><code>vpa-object-namespace</code> - A single namespace to monitor</li> </ol> <p>These options cannot be used together and are mutually exclusive.</p>"},{"location":"examples/#setting-the-webhook-failurepolicy","title":"Setting the webhook failurePolicy","text":"<p>It is possible to set the failurePolicy of the webhook to <code>Fail</code> by passing <code>--webhook-failure-policy-fail=true</code> to the VPA admission controller. Please use this option with caution as it may be possible to break Pod creation if there is a failure with the VPA. Using it in conjunction with <code>--ignored-vpa-object-namespaces=kube-system</code> or <code>--vpa-object-namespace</code> to reduce risk.</p>"},{"location":"installation/","title":"Installation","text":"<p>The current default version is Vertical Pod Autoscaler 1.2.1</p>"},{"location":"installation/#compatibility","title":"Compatibility","text":"VPA version Kubernetes version 1.2.1 1.27+ 1.2.0 1.27+ 1.1.2 1.25+ 1.1.1 1.25+ 1.0 1.25+ 0.14 1.25+ 0.13 1.25+ 0.12 1.25+ 0.11 1.22 - 1.24 0.10 1.22+ 0.9 1.16+ 0.8 1.13+ 0.4 to 0.7 1.11+ 0.3.X and lower 1.7+"},{"location":"installation/#notice-on-crd-update-100","title":"Notice on CRD update (&gt;=1.0.0)","text":"<p>NOTE: In version 1.0.0, we have updated the CRD definition and added RBAC for the status resource. If you are upgrading from version (&lt;=0.14.0), you must update the CRD definition and RBAC.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/vpa-release-1.0/vertical-pod-autoscaler/deploy/vpa-v1-crd-gen.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/vpa-release-1.0/vertical-pod-autoscaler/deploy/vpa-rbac.yaml\n</code></pre> <p>Another method is to re-execute the ./hack/vpa-process-yamls.sh script.</p> <pre><code>git clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\ngit checkout origin/vpa-release-1.0\nREGISTRY=registry.k8s.io/autoscaling TAG=1.0.0 ./hack/vpa-process-yamls.sh apply\n</code></pre> <p>If you need to roll back to version (&lt;=0.14.0), please check out the release for your rollback version and execute ./hack/vpa-process-yamls.sh. For example, to rollback to 0.14.0:</p> <pre><code>git checkout origin/vpa-release-0.14\nREGISTRY=registry.k8s.io/autoscaling TAG=0.14.0 ./hack/vpa-process-yamls.sh apply\nkubectl delete clusterrole system:vpa-status-actor\nkubectl delete clusterrolebinding system:vpa-status-actor\n</code></pre>"},{"location":"installation/#notice-on-deprecation-of-v1beta2-version-0130","title":"Notice on deprecation of v1beta2 version (&gt;=0.13.0)","text":"<p>NOTE: In 0.13.0 we deprecate <code>autoscaling.k8s.io/v1beta2</code> API. We plan to remove this API version. While for now you can continue to use <code>v1beta2</code> API we recommend using <code>autoscaling.k8s.io/v1</code> instead. <code>v1</code> and <code>v1beta2</code> APIs are almost identical (<code>v1</code> API has some fields which are not present in <code>v1beta2</code>) so simply changing which API version you're calling should be enough in almost all cases.</p>"},{"location":"installation/#notice-on-removal-of-v1beta1-version-050","title":"Notice on removal of v1beta1 version (&gt;=0.5.0)","text":"<p>NOTE: In 0.5.0 we disabled the old version of the API - <code>autoscaling.k8s.io/v1beta1</code>. The VPA objects in this version will no longer receive recommendations and existing recommendations will be removed. The objects will remain present though and a ConfigUnsupported condition will be set on them.</p> <p>This doc is for installing latest VPA. For instructions on migration from older versions see Migration Doc.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>kubectl</code> should be connected to the cluster you want to install VPA.</li> <li>The metrics server must be deployed in your cluster. Read more about Metrics Server.</li> <li>If you are using a GKE Kubernetes cluster, you will need to grant your current Google   identity <code>cluster-admin</code> role. Otherwise, you won't be authorized to grant extra   privileges to the VPA system components.</li> </ul> <pre><code>$ gcloud info | grep Account    # get current google identity\nAccount: [myname@example.org]\n\n$ kubectl create clusterrolebinding myname-cluster-admin-binding --clusterrole=cluster-admin --user=myname@example.org\nClusterrolebinding \"myname-cluster-admin-binding\" created\n</code></pre> <ul> <li>If you already have another version of VPA installed in your cluster, you have to tear down   the existing installation first with:</li> </ul> <pre><code>./hack/vpa-down.sh\n</code></pre>"},{"location":"installation/#install-command","title":"Install command","text":"<p>To install VPA, please download the source code of VPA (for example with <code>git clone https://github.com/kubernetes/autoscaler.git</code>) and run the following command inside the <code>vertical-pod-autoscaler</code> directory:</p> <pre><code>./hack/vpa-up.sh\n</code></pre> <p>Note: the script currently reads environment variables: <code>$REGISTRY</code> and <code>$TAG</code>. Make sure you leave them unset unless you want to use a non-default version of VPA.</p> <p>Note: If you are seeing following error during this step:</p> <pre><code>unknown option -addext\n</code></pre> <p>please upgrade openssl to version 1.1.1 or higher (needs to support -addext option) or use ./hack/vpa-up.sh on the 0.8 release branch.</p> <p>The script issues multiple <code>kubectl</code> commands to the cluster that insert the configuration and start all needed pods (see architecture) in the <code>kube-system</code> namespace. It also generates and uploads a secret (a CA cert) used by VPA Admission Controller when communicating with the API server.</p> <p>To print YAML contents with all resources that would be understood by <code>kubectl diff|apply|...</code> commands, you can use</p> <pre><code>./hack/vpa-process-yamls.sh print\n</code></pre> <p>The output of that command won't include secret information generated by pkg/admission-controller/gencerts.sh script.</p>"},{"location":"installation/#quick-start","title":"Quick start","text":"<p>After installation the system is ready to recommend and set resource requests for your pods. In order to use it, you need to insert a Vertical Pod Autoscaler resource for each controller that you want to have automatically computed resource requirements. This will be most commonly a Deployment. There are four modes in which VPAs operate:</p> <ul> <li><code>\"Auto\"</code>: VPA assigns resource requests on pod creation as well as updates   them on existing pods using the preferred update mechanism. Currently, this is   equivalent to <code>\"Recreate\"</code> (see below). Once restart free (\"in-place\") update   of pod requests is available, it may be used as the preferred update mechanism by   the <code>\"Auto\"</code> mode.</li> <li><code>\"Recreate\"</code>: VPA assigns resource requests on pod creation as well as updates   them on existing pods by evicting them when the requested resources differ significantly   from the new recommendation (respecting the Pod Disruption Budget, if defined).   This mode should be used rarely, only if you need to ensure that the pods are restarted   whenever the resource request changes. Otherwise, prefer the <code>\"Auto\"</code> mode which may take   advantage of restart-free updates once they are available.</li> <li><code>\"Initial\"</code>: VPA only assigns resource requests on pod creation and never changes them   later.</li> <li><code>\"Off\"</code>: VPA does not automatically change the resource requirements of the pods.   The recommendations are calculated and can be inspected in the VPA object.</li> </ul>"},{"location":"installation/#test-your-installation","title":"Test your installation","text":"<p>A simple way to check if Vertical Pod Autoscaler is fully operational in your cluster is to create a sample deployment and a corresponding VPA config:</p> <pre><code>kubectl create -f examples/hamster.yaml\n</code></pre> <p>The above command creates a deployment with two pods, each running a single container that requests 100 millicores and tries to utilize slightly above 500 millicores. The command also creates a VPA config pointing at the deployment. VPA will observe the behaviour of the pods, and after about 5 minutes, they should get updated with a higher CPU request (note that VPA does not modify the template in the deployment, but the actual requests of the pods are updated). To see VPA config and current recommended resource requests run:</p> <pre><code>kubectl describe vpa\n</code></pre> <p>Note: if your cluster has little free capacity these pods may be unable to schedule. You may need to add more nodes or adjust examples/hamster.yaml to use less CPU.</p>"},{"location":"installation/#example-vpa-configuration","title":"Example VPA configuration","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: my-app-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind:       Deployment\n    name:       my-app\n  updatePolicy:\n    updateMode: \"Auto\"\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>To diagnose problems with a VPA installation, perform the following steps:</p> <ul> <li>Check if all system components are running:</li> </ul> <pre><code>kubectl --namespace=kube-system get pods|grep vpa\n</code></pre> <p>The above command should list 3 pods (recommender, updater and admission-controller) all in state Running.</p> <ul> <li>Check if the system components log any errors.   For each of the pods returned by the previous command do:</li> </ul> <pre><code>kubectl --namespace=kube-system logs [pod name] | grep -e '^E[0-9]\\{4\\}'\n</code></pre> <ul> <li>Check that the VPA Custom Resource Definition was created:</li> </ul> <pre><code>kubectl get customresourcedefinition | grep verticalpodautoscalers\n</code></pre>"},{"location":"installation/#components-of-vpa","title":"Components of VPA","text":"<p>The project consists of 3 components:</p> <ul> <li> <p>Recommender - monitors the current and past resource consumption and, based on it,   provides recommended values for the containers' cpu and memory requests.</p> </li> <li> <p>Updater - checks which of the managed pods have correct resources set and, if not,   kills them so that they can be recreated by their controllers with the updated requests.</p> </li> <li> <p>Admission Plugin - sets the correct resource requests on new pods (either just created   or recreated by their controller due to Updater's activity).</p> </li> </ul> <p>More on the architecture can be found HERE.</p>"},{"location":"installation/#tear-down","title":"Tear down","text":"<p>Note that if you stop running VPA in your cluster, the resource requests for the pods already modified by VPA will not change, but any new pods will get resources as defined in your controllers (i.e. deployment or replicaset) and not according to previous recommendations made by VPA.</p> <p>To stop using Vertical Pod Autoscaling in your cluster:</p> <ul> <li>If running on GKE, clean up role bindings created in Prerequisites:</li> </ul> <pre><code>kubectl delete clusterrolebinding myname-cluster-admin-binding\n</code></pre> <ul> <li>Tear down VPA components:</li> </ul> <pre><code>./hack/vpa-down.sh\n</code></pre>"},{"location":"limitations/","title":"Known limitations","text":"<ul> <li>Whenever VPA updates the pod resources, the pod is recreated, which causes all   running containers to be recreated. The pod may be recreated on a different   node.</li> <li>VPA cannot guarantee that pods it evicts or deletes to apply recommendations   (when configured in <code>Auto</code> and <code>Recreate</code> modes) will be successfully   recreated. This can be partly   addressed by using VPA together with Cluster Autoscaler.</li> <li>VPA does not update resources of pods which are not run under a controller.</li> <li>Vertical Pod Autoscaler should not be used with the Horizontal Pod   Autoscaler   (HPA) on the same resource metric (CPU or memory) at this moment. However, you can use VPA with   HPA on separate resource metrics (e.g. VPA   on memory and HPA on CPU) as well as with HPA on custom and external   metrics.</li> <li>The VPA admission controller is an admission webhook. If you add other admission webhooks   to your cluster, it is important to analyze how they interact and whether they may conflict   with each other. The order of admission controllers is defined by a flag on API server.</li> <li>VPA reacts to most out-of-memory events, but not in all situations.</li> <li>VPA performance has not been tested in large clusters.</li> <li>VPA recommendation might exceed available resources (e.g. Node size, available   size, available quota) and cause pods to go pending. This can be partly   addressed by using VPA together with Cluster Autoscaler.</li> <li>Multiple VPA resources matching the same pod have undefined behavior.</li> <li>Running the vpa-recommender with leader election enabled (<code>--leader-elect=true</code>) in a GKE cluster   causes contention with a lease called <code>vpa-recommender</code> held by the GKE system component of the   same name. To run your own VPA in GKE, make sure to specify a different lease name using   <code>--leader-elect-resource-name=vpa-recommender-lease</code> (or specify your own lease name).</li> </ul>"},{"location":"limitcontrol/","title":"Limits control","text":"<p>When setting limits VPA will conform to resource policies. It will maintain limit to request ratio specified for all containers.</p> <p>VPA will try to cap recommendations between min and max of limit ranges. If limit range conflicts with VPA resource policy, VPA will follow VPA policy (and set values outside the limit range).</p> <p>To disable getting VPA recommendations for an individual container, set <code>mode</code> to <code>\"Off\"</code> in <code>containerPolicies</code>.</p>"}]}